{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data scraping with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary imports\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.expected_conditions import presence_of_element_located\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from selenium.webdriver import Firefox\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is no csv in the current folder, scrape the website and create one\n",
    "if not os.path.isfile(\"2019_paris_weather.csv\"):\n",
    "\n",
    "    # declare a dicitonnary with all required columns\n",
    "    weather_data = {\n",
    "        \"day\": [],\n",
    "        \"time\": [],\n",
    "        \"temperature\": [],\n",
    "        \"weather\": []\n",
    "    }\n",
    "\n",
    "    # use firefox as the driver\n",
    "    with Firefox() as driver:\n",
    "\n",
    "        # implicitly wait 10 seconds for any elements to be accessible\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        # go on the weather website, for every month of 2019,\n",
    "        # and accept all cookies, only for the first load\n",
    "        first_time = True\n",
    "        for month in range(1,13):\n",
    "            driver.get(f\"https://www.timeanddate.com/weather/france/paris/historic?month={month}&year=2019\")\n",
    "            if first_time:\n",
    "                driver.find_element_by_xpath(\"/html/body/div[3]/div/div/div/div[2]/div[1]/button[1]\").click()\n",
    "                first_time = False\n",
    "\n",
    "            # store the path to the scrolling menu that selects the days of the month\n",
    "            menu = driver.find_element(By.XPATH, '//*[@id=\"wt-his-select\"]')\n",
    "\n",
    "            # store the days in an iterable \"days\"\n",
    "            days = menu.find_elements(By.TAG_NAME, \"option\")\n",
    "\n",
    "            # for every day of the current month ...\n",
    "            for day in days:\n",
    "\n",
    "                # click on the day\n",
    "                day.click()\n",
    "\n",
    "                # store the table address in \"table\"\n",
    "                table = driver.find_element(By.XPATH, \"/html/body/div[6]/main/article/div[6]/div[2]/div/table/tbody\")\n",
    "\n",
    "                # store the table entries in \"entries\"\n",
    "                entries = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "                # for every entry of the table ...\n",
    "                for entry in entries:\n",
    "\n",
    "                    # add a new line to weather_data, with all required data\n",
    "                    weather_data[\"day\"].append(day.text)\n",
    "                    weather_data[\"time\"].append(entry.find_element(By.TAG_NAME, \"th\").text[:5])\n",
    "                    weather_data[\"temperature\"].append(entry.find_elements(By.TAG_NAME, \"td\")[1].text)\n",
    "                    weather_data[\"weather\"].append(entry.find_elements(By.TAG_NAME, \"td\")[2].text)\n",
    "\n",
    "    # create a pandas dataframe\n",
    "    weather_df = pd.DataFrame.from_dict(weather_data)\n",
    "    \n",
    "    # enregistre un fichier csv\n",
    "    weather_df.to_csv(\"2019_paris_weather.csv\", index=False)\n",
    "    print(\"csv generation\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    # read the csv and store the data in weather_df\n",
    "    weather_df = pd.read_csv(\"2019_paris_weather.csv\")\n",
    "    print(\"the csv already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a \"date\" column that holds the date as datetime type\n",
    "weather_df['date'] = pd.to_datetime(weather_df['day'] + \" \" + weather_df['time'])\n",
    "\n",
    "# delete every weather data from midnight to 6AM\n",
    "weather_df['hours'] = pd.to_datetime(weather_df['date']).dt.hour\n",
    "weather_df = weather_df[weather_df['hours'].between(6, 23)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit scraping using the Pushshift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary imports\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushshiftData(after, before, score_min):\n",
    "    \n",
    "    # function that scrapes the website within the time window that we want\n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?size=100&after='+str(after)+'&before='+str(before)+'&subreddit=france&score=%3E'+str(score_min)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "\n",
    "def collectSubData(subm, dict_data):\n",
    "    \n",
    "    # function that gathers all necessary data from a submission and puts it in dict_data\n",
    "    \n",
    "    dict_data[\"created_utc\"].append(int(subm[\"created_utc\"]))\n",
    "    dict_data[\"# comments\"].append(int(subm[\"num_comments\"]))\n",
    "    dict_data[\"score\"].append(int(subm[\"score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is no csv in the current folder, scrape the website and create one\n",
    "if not os.path.isfile(\"2019_best_france_post.csv\"):\n",
    "    \n",
    "    # create a dictionnary that will hold the data\n",
    "    dict_data = {\n",
    "        \"created_utc\": [],\n",
    "        \"# comments\": [],\n",
    "        \"score\": []\n",
    "    }\n",
    "    \n",
    "    # declare the time window in which to look for submissions,\n",
    "    # and the minimal score of the submissions\n",
    "    after = int(datetime(2019, 1, 1).timestamp())\n",
    "    before = int(datetime(2020, 1, 1).timestamp())\n",
    "    score_min = 500\n",
    "    print(after)\n",
    "\n",
    "    # gather data from the website\n",
    "    data = getPushshiftData(after, before, score_min)\n",
    "\n",
    "    # ask for all submissions within the time period, as the api only shows 100 submissions at a time\n",
    "    while len(data) > 0:\n",
    "        for submission in data:\n",
    "            collectSubData(submission, dict_data)\n",
    "        print(len(data))\n",
    "        after = data[-1]['created_utc']\n",
    "        time.sleep(10)\n",
    "        print(after)\n",
    "        data = getPushshiftData(after, before, score_min)\n",
    "\n",
    "    # create a pandas dataframe to hold the data\n",
    "    reddit_df = pd.DataFrame.from_dict(dict_data)\n",
    "    \n",
    "    # generate the csv file\n",
    "    print(\"csv generation\")\n",
    "    reddit_df.to_csv(\"2019_best_france_post.csv\", index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # read the data from the csv file\n",
    "    reddit_df = pd.read_csv(\"2019_best_france_post.csv\")\n",
    "    print(\"the csv already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
